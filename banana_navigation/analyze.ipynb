{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.append(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from buffer import (\n",
    "    ReplayBuffer,\n",
    "    Qentry,\n",
    "    Qcollection\n",
    ")\n",
    "\n",
    "\n",
    "from model import (\n",
    "    Qnet,\n",
    "    inference_episode,\n",
    "    train_episode,\n",
    "    inference_episode_test,\n",
    "    _state_to_torch\n",
    ")\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Unitity Enviroment that we are going to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:\n",
      "\n",
      " [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ] \n",
      "\n",
      "state size:\n",
      "\n",
      " 37 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"unity/Banana_Linux/Banana.x86_64\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "\n",
    "state_size = len(state)\n",
    "\n",
    "print(\"state:\\n\\n\", state, \"\\n\")\n",
    "print(\"state size:\\n\\n\", state_size, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the experiment run as displayed by tensorboard\n",
    "experiement_name = \"final\"\n",
    "\n",
    "# The number of training iterations\n",
    "iterations = 10000\n",
    "\n",
    "# The learning-rate used during training of the model\n",
    "lr = 0.001\n",
    "\n",
    "# The size of the replay buffer\n",
    "replay_buffer_size = 20000\n",
    "\n",
    "# The batchsize for each update run\n",
    "batch_size = 128\n",
    "# How many consecutive update runs are we going to do before we are producing new entries to the replaybuffer\n",
    "update_steps = 20\n",
    "# Controls the weighing of old entries to the replay buffer\n",
    "# We normalize the weights of the entries in the replay buffer by the number times we have used the\n",
    "# the entry to train the model and use sampling_beta as the exponent to that normalizing factor. \n",
    "# i.e. sampling_beta -> inf, we only care about new entries, sampling_beta = 0, we do not care of we have seen the \n",
    "# entry many times we will sample the entry. \n",
    "sampling_beta = 1\n",
    "\n",
    "# The probability that we will choose one action on random instead of selecting the action with the highest q-value \n",
    "epsilon=1.0\n",
    "# Controls how we are decreasing epsilon during training\n",
    "eps_decay=0.999 \n",
    "# The final epsilon\n",
    "eps_end = 0.01\n",
    "\n",
    "# Discount factor in the temporal difference q-learning loss, controls the focus on future vs immediate reward \n",
    "gamma=0.99\n",
    "\n",
    "# lowpass filter factor that controls the q-learning's targets update speed in terms\n",
    "copy_weight_scale = 0.0075\n",
    "\n",
    "# How many episodes are we using when evaluating the performance of the model\n",
    "test_iter = 100\n",
    "# How frequently are we running a validation-run during training\n",
    "validation_iter = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Setting of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet = Qnet(state_size, action_size, lr=lr)\n",
    "\n",
    "qnet_target = Qnet(state_size, action_size, lr=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the replaybuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaybuffer = ReplayBuffer(replay_buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the tensorboard writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f\"runs/{experiement_name}\")\n",
    "\n",
    "writer.add_graph(qnet, _state_to_torch(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a previous trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"models/model.ckp\" is the best verified model\n",
    "qnet.load_state_dict(torch.load(\"models/model.ckp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 500 - score: 2.04\n",
      "iteration: 1000 - score: 0.83\n",
      "iteration: 1500 - score: 2.59\n",
      "iteration: 2000 - score: 7.96\n",
      "iteration: 2500 - score: 7.41\n",
      "iteration: 3000 - score: 8.96\n",
      "iteration: 3500 - score: 11.08\n",
      "iteration: 4000 - score: 13.18\n",
      "iteration: 4500 - score: 14.73\n",
      "iteration: 5000 - score: 13.38\n",
      "iteration: 5500 - score: 10.24\n",
      "iteration: 6000 - score: 14.83\n",
      "iteration: 6500 - score: 13.61\n",
      "iteration: 7000 - score: 10.89\n",
      "iteration: 7500 - score: 14.23\n",
      "iteration: 8000 - score: 15.58\n",
      "iteration: 8500 - score: 10.91\n",
      "iteration: 9000 - score: 12.49\n",
      "iteration: 9500 - score: 14.2\n"
     ]
    }
   ],
   "source": [
    "best_test_score = -np.inf\n",
    "\n",
    "for iteration in range(1, iterations):\n",
    "    \n",
    "    epsilon = max(eps_end, eps_decay*epsilon) # decrease epsilon\n",
    "    \n",
    "    # Do inference for one episode and store the results in the replaybuffer\n",
    "    _param_dict = {}\n",
    "    _param_dict[\"brain_name\"] = brain_name\n",
    "    _param_dict[\"gamma\"] = gamma\n",
    "    _param_dict[\"beta\"] = sampling_beta\n",
    "    _param_dict[\"reward_default\"] = 0\n",
    "    _param_dict[\"epsilon\"] = epsilon\n",
    "\n",
    "    replaybuffer, score, hits = inference_episode(\n",
    "        meta=_param_dict,\n",
    "        env=env,\n",
    "        qnet=qnet,\n",
    "        replaybuffer=replaybuffer\n",
    "    )\n",
    "    \n",
    "    # Train the network using the stored entries in the replaybuffer\n",
    "    _param_dict = {}\n",
    "    _param_dict[\"batch_size\"] = batch_size\n",
    "    _param_dict[\"gamma\"] = gamma\n",
    "    _param_dict[\"update_steps\"] = update_steps\n",
    "    _param_dict[\"replace_sampling\"] = False\n",
    "    _param_dict[\"beta\"] = sampling_beta\n",
    "\n",
    "    qvalues_target, action = train_episode(\n",
    "        meta=_param_dict,\n",
    "        qnet=qnet,\n",
    "        qnet_target=qnet_target,\n",
    "        replaybuffer=replaybuffer,\n",
    "    )\n",
    "    \n",
    "    # Write some statitics to tensorboard\n",
    "    writer.add_scalar(\"training/kpi/score\", score, iteration) # The score\n",
    "    writer.add_scalar(\"training/kpi/hits\", hits, iteration) # Number of bannanas taken (blue or yellow)\n",
    "    writer.add_scalar(\"training/hyper/epsilon\", epsilon, iteration) # Epsilon\n",
    "    \n",
    "    # Update the target network\n",
    "    qnet_target.steal_weights(qnet, copy_weight_scale)\n",
    "    \n",
    "    # Validate the network \n",
    "    if iteration % validation_iter == 0:\n",
    "        \n",
    "        _param_dict = {}\n",
    "        _param_dict[\"brain_name\"] = brain_name\n",
    "        _param_dict[\"iterations\"] = test_iter\n",
    "        \n",
    "        test_score = inference_episode_test(\n",
    "            meta=_param_dict,\n",
    "            env=env,\n",
    "            qnet=qnet,\n",
    "            train_mode=True,\n",
    "        )\n",
    "        \n",
    "        print(f\"iteration: {iteration} - score: {test_score}\")\n",
    "        \n",
    "        writer.add_scalar(\"validation/kpi/score\", test_score, iteration)\n",
    "        \n",
    "        # Create a checkpoing for the best model, according to the test score\n",
    "        if test_score > best_test_score:\n",
    "            best_test_score = test_score\n",
    "            torch.save(qnet.state_dict(), f\"models/{experiement_name}.ckp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnet.load_state_dict(torch.load(f\"models/{experiement_name}.ckp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.55"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_param_dict = {}\n",
    "_param_dict[\"brain_name\"] = brain_name\n",
    "_param_dict[\"iterations\"] = test_iter\n",
    "\n",
    "test_score = inference_episode_test(\n",
    "    meta=_param_dict,\n",
    "    env=env,\n",
    "    qnet=qnet,\n",
    "    train_mode=True,\n",
    ")\n",
    "\n",
    "writer.add_hparams(\n",
    "    hparam_dict = {\n",
    "        \"replay_buffer_size\": replay_buffer_size,\n",
    "        \"epsilon_start\": epsilon,\n",
    "        \"eps_end\": eps_end,\n",
    "        \"eps_decay\": eps_decay,\n",
    "        \"gamma\": gamma,\n",
    "        \"copy_weight_scale\": copy_weight_scale,\n",
    "    },\n",
    "    metric_dict={\"score\": test_score},\n",
    "    run_name=\"final_score\"\n",
    ")\n",
    "test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the stored primary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(qnet.state_dict(), \"models/model.ckp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
