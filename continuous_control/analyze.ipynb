{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "from model import (\n",
    "    Critic,\n",
    "    Actor,\n",
    "    DDPG,\n",
    ")\n",
    "\n",
    "from buffer import ReplayBuffer\n",
    "from noise import OUNoise\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from environment_interaction import (\n",
    "    evaluation,\n",
    "    train,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about the enviroment that we are going to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20 \n",
      "\n",
      "Size of each action: 4 \n",
      "\n",
      "There are 20 agents. Each observes a state with length: 33 \n",
      "\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='unity/Reacher_Linux_Many/Reacher.x86_64')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment \n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents, \"\\n\")\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size, \"\\n\")\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size), \"\\n\")\n",
    "\n",
    "print('The state for the first agent looks like:', states[0], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the experiment, as seen in tensorboard\n",
    "experiement_name = \"final\"\n",
    "\n",
    "# Size of the batches\n",
    "batch_size = 128\n",
    "# Size of the replay bufffer\n",
    "replay_buffer_size = int(1e6)\n",
    "\n",
    "# In the replay buffer we normalize the weights (w), associated with each replay tuplet, with the number of times (seen),\n",
    "# the tuplet has been used for training. We then scale (w) with (seen) -> w <- w / seen ** beta.\n",
    "# High beta -> we prefer to sample unseen tuples \n",
    "beta = 1\n",
    "\n",
    "# In the replay buffer, after we have scaled the weights and converted it to a probability distribution (p) we scale the \n",
    "# distribution, p.^beta_2. High beta_2 -> we will only sample tuples with high weight\n",
    "beta_2 = 2\n",
    "\n",
    "# Number of episodes used for training\n",
    "episodes = 2000\n",
    "\n",
    "# Gamma in the td-loss \n",
    "gamma = 0.99\n",
    "\n",
    "# We have the possibility of scaling the reward with a factor\n",
    "scale_reward = 10\n",
    "\n",
    "# Where do you want to run the inference/training, cuda or cpu \n",
    "device = \"cuda\"\n",
    "\n",
    "# We wait (inference_steps), for each training episode, before we update the models\n",
    "inference_steps = 20\n",
    "# When we start update the models we do that for (update_steps) \n",
    "update_steps = 45\n",
    "# Total number of training steps per episode (1000 / inference_steps) * update_steps\n",
    "\n",
    "# We randomly, with probability sigma, choose to use action generated from our noise-generator or to use \n",
    "# the output from the actor-network\n",
    "\n",
    "# Start of sigma\n",
    "sigma_init = 1\n",
    "# decay factor\n",
    "sigma_decay = 0.86\n",
    "# stop of sigma\n",
    "sigma_end = 0.1\n",
    "\n",
    "# Ornstein–Uhlenbeck sigma \n",
    "ou_sigma = 0.25\n",
    "\n",
    "# Number of times we are running evaluation episodes (that we average over) after each training episode\n",
    "eval_rounds = 1\n",
    "\n",
    "# Learning rates\n",
    "lr_critic = 10**-4\n",
    "lr_actor = 10**-4\n",
    "# Controls how we are updating the target network after each training step\n",
    "tau_critic = 10**-3\n",
    "tau_actor = 10**-3\n",
    "\n",
    "# Number of output nodes in the first 2 layers\n",
    "hidden_dim = 256\n",
    "\n",
    "# Number of output nodes in the following hidden layers\n",
    "squeeze_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(\n",
    "    in_dim=state_size,\n",
    "    out_dim=action_size,\n",
    "    hidden_dim=hidden_dim,\n",
    "    squeeze_dim=squeeze_dim,\n",
    ")        \n",
    "\n",
    "critic = Critic(\n",
    "    in_dim=state_size+action_size,\n",
    "    out_dim=1,\n",
    "    hidden_dim=hidden_dim,\n",
    "    squeeze_dim=squeeze_dim,\n",
    ")        \n",
    "\n",
    "# The targets start out as copies of the networks that will undergoe training \n",
    "actor_target = actor.copy()\n",
    "critic_target = critic.copy()\n",
    "\n",
    "ddpg = DDPG(\n",
    "    critic=critic,\n",
    "    actor=actor,\n",
    "    critic_target=critic_target,\n",
    "    actor_target=actor_target,\n",
    "    lr_critic=lr_critic,\n",
    "    lr_actor=lr_actor,\n",
    "    tau_critic=tau_critic,\n",
    "    tau_actor=tau_actor,\n",
    ")\n",
    "\n",
    "replaybuffer = ReplayBuffer(\n",
    "    replay_buffer_size,\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    ")\n",
    "\n",
    "# Send the models to the device of choice\n",
    "ddpg = ddpg.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting up the tensorboard writer\n",
    "writer = SummaryWriter(f\"continuous_control/runs/{experiement_name}\")\n",
    "\n",
    "# Setting up the Ornstein–Uhlenbeck noise generator \n",
    "ounoise = OUNoise(\n",
    "    n=len(env_info.agents),\n",
    "    n_actions=action_size,\n",
    "    mu=0,\n",
    "    theta=0.15,\n",
    "    sigma=ou_sigma,\n",
    "    low=-1,\n",
    "    high=1,\n",
    ")\n",
    "\n",
    "# Train the models\n",
    "train(\n",
    "    env=env,\n",
    "    ddpg=ddpg,\n",
    "    replaybuffer=replaybuffer,\n",
    "    ounoise=ounoise, \n",
    "    writer=writer,\n",
    "    episodes=episodes,\n",
    "    brain_name=brain_name,\n",
    "    device=device,\n",
    "    batch_size=batch_size,\n",
    "    gamma=gamma,\n",
    "    sigma_init=sigma_init,\n",
    "    sigma_end=sigma_end,\n",
    "    sigma_decay=sigma_decay,\n",
    "    evaluation_rounds=eval_rounds,\n",
    "    train_mode=True,\n",
    "    inference_steps=inference_steps,\n",
    "    update_steps=update_steps,\n",
    "    beta=beta,\n",
    "    beta_2=beta_2,\n",
    "    scale_reward=scale_reward,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoints of trainied models\n",
    "ddpg._actor.load_state_dict(torch.load(\"models/actor.ckp\"))\n",
    "ddpg._critic.load_state_dict(torch.load(\"models/critic.ckp\"))\n",
    "\n",
    "score = evaluation(\n",
    "    env=env,\n",
    "    ddpg=ddpg,\n",
    "    episodes=100,\n",
    "    device=device,\n",
    "    brain_name=brain_name,\n",
    "    train_mode=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.43875911847502\n"
     ]
    }
   ],
   "source": [
    "# The score\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
