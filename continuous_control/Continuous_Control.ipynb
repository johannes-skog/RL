{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "work_dir = os.getcwd()\n",
    "\n",
    "sys.path.insert(0, 'continuous_control/src')\n",
    "\n",
    "from model import (\n",
    "    Critic,\n",
    "    Actor,\n",
    "    DDPG,\n",
    ")\n",
    "\n",
    "from util import _state_to_torch\n",
    "\n",
    "from utils.buffer import ReplayBuffer\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "env = UnityEnvironment(file_name='continuous_control/unity/Reacher_Linux_Many/Reacher.x86_64')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaybuffer = ReplayBuffer(\n",
    "    5000,\n",
    "    state_size=30,\n",
    "    action_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "0.0034160614013671875\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    state = np.random.rand(20, 30)\n",
    "    state_prime = np.random.rand(20, 30)\n",
    "    action = np.random.rand(20, 4)\n",
    "    reward = np.random.rand(20)\n",
    "    weight = np.random.rand(20)\n",
    "    done = np.random.rand(20)\n",
    "\n",
    "    replaybuffer.add(\n",
    "        state=state,\n",
    "        state_prime=state_prime,\n",
    "        reward=reward,\n",
    "        action=action,\n",
    "        weight=weight,\n",
    "        done=done\n",
    "    )\n",
    "    \n",
    "    print(len(replaybuffer))\n",
    "    \n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33284286, 0.75338119, 0.04289542, 0.53129129],\n",
       "       [0.2982976 , 0.11867255, 0.47619062, 0.2646239 ],\n",
       "       [0.47743836, 0.74935759, 0.60646312, 0.53024298],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaybuffer._action[0:4990, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    state, \n",
    "    state_prime,\n",
    "    action,\n",
    "    reward,\n",
    "    done, \n",
    "    weight\n",
    ") = replaybuffer.draw(201)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replaybuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(\n",
    "    in_dim=state_size,\n",
    "    out_dim=action_size,\n",
    "    hidden_dim=128,\n",
    "    squeeze_dim=64,\n",
    "    res_block=5,\n",
    "    limit_l=-1,\n",
    "    limit_h=1,\n",
    ")        \n",
    "\n",
    "critic = Critic(\n",
    "    in_dim=state_size+action_size,\n",
    "    out_dim=1,\n",
    "    hidden_dim=128,\n",
    "    squeeze_dim=64,\n",
    "    res_block=5,\n",
    ")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.load_state_dict(torch.load(\"models/actor.ckp\"))\n",
    "critic.load_state_dict(torch.load(\"models/critic.ckp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Deterministic Policy Gradient Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg = DDPG(\n",
    "    critic=critic,\n",
    "    actor=actor,\n",
    "    lr_critic=10**-3,\n",
    "    lr_actor=10**-3,\n",
    "    tau_critic=10**-3,\n",
    "    tau_actor=10**-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Setting up the replaybuffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaybuffer = ReplayBuffer(\n",
    "    10000,\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiement_name = \"test\"\n",
    "\n",
    "episodes = 2000\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "gamma = 0.95\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "update_steps = 10\n",
    "\n",
    "sigma_end = 0.1\n",
    "sigma_decay = 0.99\n",
    "sigma = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the tensorboard writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f\"continuous_control/runs/{experiement_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        , -1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 0.93134606,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        , -1.        ,  1.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.495\n",
      "1 0.49005\n",
      "2 0.48514949999999996\n",
      "3 0.480298005\n",
      "4 0.47549502494999996\n",
      "5 0.47074007470049994\n",
      "6 0.46603267395349496\n",
      "7 0.46137234721396\n",
      "8 0.45675862374182036\n",
      "9 0.45219103750440215\n",
      "10 0.44766912712935814\n",
      "11 0.44319243585806456\n",
      "12 0.4387605114994839\n",
      "13 0.43437290638448905\n",
      "14 0.43002917732064416\n",
      "15 0.4257288855474377\n",
      "16 0.4214715966919633\n",
      "17 0.4172568807250437\n",
      "18 0.41308431191779327\n",
      "19 0.4089534687986153\n",
      "20 0.40486393411062915\n",
      "21 0.4008152947695229\n",
      "22 0.39680714182182764\n",
      "23 0.3928390704036094\n",
      "24 0.38891067969957327\n",
      "25 0.38502157290257755\n",
      "26 0.3811713571735518\n",
      "27 0.37735964360181623\n",
      "28 0.3735860471657981\n",
      "29 0.3698501866941401\n",
      "30 0.3661516848271987\n",
      "31 0.3624901679789267\n",
      "32 0.3588652662991374\n",
      "33 0.35527661363614604\n",
      "34 0.3517238474997846\n",
      "35 0.34820660902478673\n",
      "36 0.34472454293453886\n",
      "37 0.3412772975051935\n",
      "38 0.33786452453014154\n",
      "39 0.33448587928484014\n",
      "40 0.33114102049199173\n",
      "41 0.3278296102870718\n",
      "42 0.3245513141842011\n",
      "43 0.32130580104235906\n",
      "44 0.31809274303193547\n",
      "45 0.31491181560161613\n",
      "46 0.3117626974456\n",
      "47 0.308645070471144\n",
      "48 0.30555861976643256\n",
      "49 0.30250303356876823\n",
      "50 0.29947800323308055\n",
      "51 0.2964832232007497\n",
      "52 0.2935183909687422\n",
      "53 0.29058320705905477\n",
      "54 0.28767737498846424\n",
      "55 0.2848006012385796\n",
      "56 0.2819525952261938\n",
      "57 0.2791330692739319\n",
      "58 0.27634173858119254\n",
      "59 0.2735783211953806\n",
      "60 0.2708425379834268\n",
      "61 0.2681341126035925\n",
      "62 0.2654527714775566\n",
      "63 0.262798243762781\n",
      "64 0.2601702613251532\n",
      "65 0.25756855871190165\n",
      "66 0.25499287312478264\n",
      "67 0.2524429443935348\n",
      "68 0.24991851494959946\n",
      "69 0.24741932980010348\n",
      "70 0.24494513650210245\n",
      "71 0.24249568513708142\n",
      "72 0.2400707282857106\n",
      "73 0.2376700210028535\n",
      "74 0.23529332079282497\n",
      "75 0.2329403875848967\n",
      "76 0.23061098370904776\n",
      "77 0.22830487387195728\n",
      "78 0.2260218251332377\n",
      "79 0.22376160688190533\n",
      "80 0.22152399081308627\n",
      "81 0.2193087509049554\n",
      "82 0.21711566339590585\n",
      "83 0.2149445067619468\n",
      "84 0.21279506169432733\n",
      "85 0.21066711107738406\n",
      "86 0.20856043996661022\n",
      "87 0.20647483556694413\n",
      "88 0.20441008721127468\n",
      "89 0.20236598633916195\n",
      "90 0.20034232647577033\n",
      "91 0.19833890321101263\n",
      "92 0.1963555141789025\n",
      "93 0.19439195903711348\n",
      "94 0.19244803944674235\n",
      "95 0.19052355905227492\n",
      "96 0.18861832346175217\n",
      "97 0.18673214022713464\n",
      "98 0.18486481882486328\n",
      "99 0.18301617063661463\n",
      "100 0.18118600893024847\n",
      "101 0.179374148840946\n",
      "102 0.17758040735253652\n",
      "103 0.17580460327901115\n",
      "104 0.17404655724622103\n",
      "105 0.17230609167375882\n",
      "106 0.17058303075702122\n",
      "107 0.168877200449451\n",
      "108 0.1671884284449565\n",
      "109 0.16551654416050693\n",
      "110 0.16386137871890186\n",
      "111 0.16222276493171284\n",
      "112 0.1606005372823957\n",
      "113 0.15899453190957175\n",
      "114 0.15740458659047601\n",
      "115 0.15583054072457125\n",
      "116 0.15427223531732553\n",
      "117 0.15272951296415227\n",
      "118 0.15120221783451074\n",
      "119 0.14969019565616562\n",
      "120 0.14819329369960396\n",
      "121 0.14671136076260793\n",
      "122 0.14524424715498185\n",
      "123 0.14379180468343203\n",
      "124 0.1423538866365977\n",
      "125 0.14093034777023172\n",
      "126 0.1395210442925294\n",
      "127 0.1381258338496041\n",
      "128 0.13674457551110808\n",
      "129 0.135377129755997\n",
      "130 0.13402335845843702\n",
      "131 0.13268312487385264\n",
      "132 0.1313562936251141\n",
      "133 0.13004273068886296\n",
      "134 0.12874230338197434\n",
      "135 0.1274548803481546\n",
      "136 0.12618033154467306\n",
      "137 0.12491852822922633\n",
      "138 0.12366934294693407\n",
      "139 0.12243264951746473\n",
      "140 0.12120832302229008\n",
      "141 0.11999623979206718\n",
      "142 0.11879627739414651\n",
      "143 0.11760831462020505\n",
      "144 0.116432231474003\n",
      "145 0.11526790915926297\n",
      "146 0.11411523006767034\n",
      "147 0.11297407776699364\n",
      "148 0.11184433698932371\n",
      "149 0.11072589361943047\n",
      "150 0.10961863468323617\n",
      "151 0.1085224483364038\n",
      "152 0.10743722385303976\n",
      "153 0.10636285161450937\n",
      "154 0.10529922309836427\n",
      "155 0.10424623086738063\n",
      "156 0.10320376855870683\n",
      "157 0.10217173087311976\n",
      "158 0.10115001356438856\n",
      "159 0.10013851342874468\n",
      "160 0.1\n",
      "161 0.1\n",
      "162 0.1\n",
      "163 0.1\n",
      "164 0.1\n",
      "165 0.1\n",
      "166 0.1\n",
      "167 0.1\n",
      "168 0.1\n",
      "169 0.1\n",
      "170 0.1\n",
      "171 0.1\n",
      "172 0.1\n",
      "173 0.1\n",
      "174 0.1\n",
      "175 0.1\n",
      "176 0.1\n",
      "177 0.1\n",
      "178 0.1\n",
      "179 0.1\n",
      "180 0.1\n",
      "181 0.1\n",
      "182 0.1\n",
      "183 0.1\n",
      "184 0.1\n",
      "185 0.1\n",
      "186 0.1\n",
      "187 0.1\n",
      "188 0.1\n",
      "189 0.1\n",
      "190 0.1\n",
      "191 0.1\n",
      "192 0.1\n",
      "193 0.1\n",
      "194 0.1\n",
      "195 0.1\n",
      "196 0.1\n",
      "197 0.1\n",
      "198 0.1\n",
      "199 0.1\n",
      "200 0.1\n",
      "201 0.1\n",
      "202 0.1\n",
      "203 0.1\n",
      "204 0.1\n",
      "205 0.1\n",
      "206 0.1\n",
      "207 0.1\n",
      "208 0.1\n",
      "209 0.1\n",
      "210 0.1\n",
      "211 0.1\n",
      "212 0.1\n",
      "213 0.1\n",
      "214 0.1\n",
      "215 0.1\n",
      "216 0.1\n",
      "217 0.1\n",
      "218 0.1\n",
      "219 0.1\n",
      "220 0.1\n",
      "221 0.1\n",
      "222 0.1\n",
      "223 0.1\n",
      "224 0.1\n",
      "225 0.1\n",
      "226 0.1\n",
      "227 0.1\n",
      "228 0.1\n",
      "229 0.1\n",
      "230 0.1\n",
      "231 0.1\n",
      "232 0.1\n",
      "233 0.1\n",
      "234 0.1\n",
      "235 0.1\n",
      "236 0.1\n",
      "237 0.1\n",
      "238 0.1\n",
      "239 0.1\n",
      "240 0.1\n",
      "241 0.1\n",
      "242 0.1\n",
      "243 0.1\n",
      "244 0.1\n",
      "245 0.1\n",
      "246 0.1\n",
      "247 0.1\n",
      "248 0.1\n",
      "249 0.1\n",
      "250 0.1\n",
      "251 0.1\n",
      "252 0.1\n",
      "253 0.1\n",
      "254 0.1\n",
      "255 0.1\n",
      "256 0.1\n",
      "257 0.1\n",
      "258 0.1\n",
      "259 0.1\n",
      "260 0.1\n",
      "261 0.1\n",
      "262 0.1\n",
      "263 0.1\n",
      "264 0.1\n",
      "265 0.1\n",
      "266 0.1\n",
      "267 0.1\n",
      "268 0.1\n",
      "269 0.1\n",
      "270 0.1\n",
      "271 0.1\n",
      "272 0.1\n",
      "273 0.1\n",
      "274 0.1\n",
      "275 0.1\n",
      "276 0.1\n",
      "277 0.1\n",
      "278 0.1\n",
      "279 0.1\n",
      "280 0.1\n",
      "281 0.1\n",
      "282 0.1\n",
      "283 0.1\n",
      "284 0.1\n",
      "285 0.1\n",
      "286 0.1\n",
      "287 0.1\n",
      "288 0.1\n",
      "289 0.1\n",
      "290 0.1\n",
      "291 0.1\n",
      "292 0.1\n",
      "293 0.1\n",
      "294 0.1\n",
      "295 0.1\n",
      "296 0.1\n",
      "297 0.1\n",
      "298 0.1\n",
      "299 0.1\n",
      "300 0.1\n",
      "301 0.1\n",
      "302 0.1\n",
      "303 0.1\n",
      "304 0.1\n",
      "305 0.1\n",
      "306 0.1\n",
      "307 0.1\n",
      "308 0.1\n",
      "309 0.1\n",
      "310 0.1\n",
      "311 0.1\n",
      "312 0.1\n",
      "313 0.1\n",
      "314 0.1\n",
      "315 0.1\n",
      "316 0.1\n",
      "317 0.1\n",
      "318 0.1\n",
      "319 0.1\n",
      "320 0.1\n",
      "321 0.1\n",
      "322 0.1\n",
      "323 0.1\n",
      "324 0.1\n",
      "325 0.1\n",
      "326 0.1\n",
      "327 0.1\n",
      "328 0.1\n",
      "329 0.1\n",
      "330 0.1\n",
      "331 0.1\n",
      "332 0.1\n",
      "333 0.1\n",
      "334 0.1\n",
      "335 0.1\n",
      "336 0.1\n",
      "337 0.1\n",
      "338 0.1\n",
      "339 0.1\n",
      "340 0.1\n",
      "341 0.1\n",
      "342 0.1\n",
      "343 0.1\n",
      "344 0.1\n",
      "345 0.1\n",
      "346 0.1\n",
      "347 0.1\n",
      "348 0.1\n",
      "349 0.1\n",
      "350 0.1\n",
      "351 0.1\n",
      "352 0.1\n",
      "353 0.1\n",
      "354 0.1\n",
      "355 0.1\n",
      "356 0.1\n",
      "357 0.1\n",
      "358 0.1\n",
      "359 0.1\n",
      "360 0.1\n",
      "361 0.1\n",
      "362 0.1\n",
      "363 0.1\n",
      "364 0.1\n",
      "365 0.1\n",
      "366 0.1\n",
      "367 0.1\n",
      "368 0.1\n",
      "369 0.1\n",
      "370 0.1\n",
      "371 0.1\n",
      "372 0.1\n",
      "373 0.1\n",
      "374 0.1\n",
      "375 0.1\n",
      "376 0.1\n",
      "377 0.1\n",
      "378 0.1\n",
      "379 0.1\n",
      "380 0.1\n",
      "381 0.1\n",
      "382 0.1\n",
      "383 0.1\n",
      "384 0.1\n",
      "385 0.1\n",
      "386 0.1\n",
      "387 0.1\n",
      "388 0.1\n",
      "389 0.1\n",
      "390 0.1\n",
      "391 0.1\n",
      "392 0.1\n",
      "393 0.1\n",
      "394 0.1\n",
      "395 0.1\n",
      "396 0.1\n",
      "397 0.1\n",
      "398 0.1\n",
      "399 0.1\n",
      "400 0.1\n",
      "401 0.1\n",
      "402 0.1\n",
      "403 0.1\n",
      "404 0.1\n",
      "405 0.1\n",
      "406 0.1\n",
      "407 0.1\n",
      "408 0.1\n",
      "409 0.1\n",
      "410 0.1\n",
      "411 0.1\n",
      "412 0.1\n",
      "413 0.1\n",
      "414 0.1\n",
      "415 0.1\n",
      "416 0.1\n",
      "417 0.1\n",
      "418 0.1\n",
      "419 0.1\n",
      "420 0.1\n",
      "421 0.1\n",
      "422 0.1\n",
      "423 0.1\n",
      "424 0.1\n",
      "425 0.1\n",
      "426 0.1\n",
      "427 0.1\n",
      "428 0.1\n",
      "429 0.1\n",
      "430 0.1\n",
      "431 0.1\n",
      "432 0.1\n",
      "433 0.1\n",
      "434 0.1\n",
      "435 0.1\n",
      "436 0.1\n",
      "437 0.1\n",
      "438 0.1\n",
      "439 0.1\n",
      "440 0.1\n",
      "441 0.1\n",
      "442 0.1\n",
      "443 0.1\n",
      "444 0.1\n",
      "445 0.1\n",
      "446 0.1\n",
      "447 0.1\n",
      "448 0.1\n",
      "449 0.1\n",
      "450 0.1\n",
      "451 0.1\n",
      "452 0.1\n",
      "453 0.1\n",
      "454 0.1\n",
      "455 0.1\n",
      "456 0.1\n",
      "457 0.1\n",
      "458 0.1\n",
      "459 0.1\n",
      "460 0.1\n",
      "461 0.1\n",
      "462 0.1\n",
      "463 0.1\n",
      "464 0.1\n",
      "465 0.1\n",
      "466 0.1\n",
      "467 0.1\n",
      "468 0.1\n",
      "469 0.1\n",
      "470 0.1\n",
      "471 0.1\n",
      "472 0.1\n",
      "473 0.1\n",
      "474 0.1\n",
      "475 0.1\n",
      "476 0.1\n",
      "477 0.1\n",
      "478 0.1\n",
      "479 0.1\n",
      "480 0.1\n",
      "481 0.1\n",
      "482 0.1\n",
      "483 0.1\n",
      "484 0.1\n",
      "485 0.1\n",
      "486 0.1\n",
      "487 0.1\n",
      "488 0.1\n",
      "489 0.1\n",
      "490 0.1\n",
      "491 0.1\n",
      "492 0.1\n",
      "493 0.1\n",
      "494 0.1\n",
      "495 0.1\n",
      "496 0.1\n",
      "497 0.1\n",
      "498 0.1\n",
      "499 0.1\n",
      "500 0.1\n",
      "501 0.1\n",
      "502 0.1\n",
      "503 0.1\n",
      "504 0.1\n",
      "505 0.1\n",
      "506 0.1\n",
      "507 0.1\n",
      "508 0.1\n",
      "509 0.1\n",
      "510 0.1\n",
      "511 0.1\n",
      "512 0.1\n",
      "513 0.1\n",
      "514 0.1\n",
      "515 0.1\n",
      "516 0.1\n",
      "517 0.1\n",
      "518 0.1\n",
      "519 0.1\n",
      "520 0.1\n",
      "521 0.1\n",
      "522 0.1\n",
      "523 0.1\n",
      "524 0.1\n",
      "525 0.1\n",
      "526 0.1\n",
      "527 0.1\n",
      "528 0.1\n",
      "529 0.1\n",
      "530 0.1\n",
      "531 0.1\n",
      "532 0.1\n",
      "533 0.1\n",
      "534 0.1\n",
      "535 0.1\n",
      "536 0.1\n",
      "537 0.1\n",
      "538 0.1\n",
      "539 0.1\n",
      "540 0.1\n",
      "541 0.1\n",
      "542 0.1\n",
      "543 0.1\n",
      "544 0.1\n",
      "545 0.1\n",
      "546 0.1\n",
      "547 0.1\n",
      "548 0.1\n",
      "549 0.1\n",
      "550 0.1\n",
      "551 0.1\n",
      "552 0.1\n",
      "553 0.1\n",
      "554 0.1\n",
      "555 0.1\n",
      "556 0.1\n",
      "557 0.1\n",
      "558 0.1\n",
      "559 0.1\n",
      "560 0.1\n",
      "561 0.1\n",
      "562 0.1\n",
      "563 0.1\n",
      "564 0.1\n",
      "565 0.1\n",
      "566 0.1\n",
      "567 0.1\n",
      "568 0.1\n",
      "569 0.1\n",
      "570 0.1\n",
      "571 0.1\n",
      "572 0.1\n",
      "573 0.1\n",
      "574 0.1\n",
      "575 0.1\n",
      "576 0.1\n",
      "577 0.1\n",
      "578 0.1\n",
      "579 0.1\n",
      "580 0.1\n",
      "581 0.1\n",
      "582 0.1\n",
      "583 0.1\n",
      "584 0.1\n",
      "585 0.1\n",
      "586 0.1\n",
      "587 0.1\n",
      "588 0.1\n",
      "589 0.1\n",
      "590 0.1\n",
      "591 0.1\n",
      "592 0.1\n",
      "593 0.1\n",
      "594 0.1\n",
      "595 0.1\n",
      "596 0.1\n",
      "597 0.1\n",
      "598 0.1\n",
      "599 0.1\n",
      "600 0.1\n",
      "601 0.1\n",
      "602 0.1\n",
      "603 0.1\n",
      "604 0.1\n",
      "605 0.1\n",
      "606 0.1\n",
      "607 0.1\n",
      "608 0.1\n",
      "609 0.1\n",
      "610 0.1\n",
      "611 0.1\n",
      "612 0.1\n",
      "613 0.1\n",
      "614 0.1\n",
      "615 0.1\n",
      "616 0.1\n",
      "617 0.1\n",
      "618 0.1\n",
      "619 0.1\n",
      "620 0.1\n",
      "621 0.1\n",
      "622 0.1\n",
      "623 0.1\n",
      "624 0.1\n",
      "625 0.1\n",
      "626 0.1\n",
      "627 0.1\n",
      "628 0.1\n",
      "629 0.1\n",
      "630 0.1\n",
      "631 0.1\n",
      "632 0.1\n",
      "633 0.1\n",
      "634 0.1\n",
      "635 0.1\n",
      "636 0.1\n",
      "637 0.1\n",
      "638 0.1\n",
      "639 0.1\n",
      "640 0.1\n",
      "641 0.1\n",
      "642 0.1\n",
      "643 0.1\n",
      "644 0.1\n",
      "645 0.1\n",
      "646 0.1\n",
      "647 0.1\n",
      "648 0.1\n",
      "649 0.1\n",
      "650 0.1\n",
      "651 0.1\n",
      "652 0.1\n",
      "653 0.1\n",
      "654 0.1\n",
      "655 0.1\n",
      "656 0.1\n",
      "657 0.1\n",
      "658 0.1\n",
      "659 0.1\n",
      "660 0.1\n",
      "661 0.1\n",
      "662 0.1\n",
      "663 0.1\n",
      "664 0.1\n",
      "665 0.1\n",
      "666 0.1\n",
      "667 0.1\n",
      "668 0.1\n",
      "669 0.1\n",
      "670 0.1\n",
      "671 0.1\n",
      "672 0.1\n",
      "673 0.1\n",
      "674 0.1\n",
      "675 0.1\n",
      "676 0.1\n",
      "677 0.1\n",
      "678 0.1\n",
      "679 0.1\n",
      "680 0.1\n",
      "681 0.1\n",
      "682 0.1\n",
      "683 0.1\n",
      "684 0.1\n",
      "685 0.1\n",
      "686 0.1\n",
      "687 0.1\n",
      "688 0.1\n",
      "689 0.1\n",
      "690 0.1\n",
      "691 0.1\n",
      "692 0.1\n",
      "693 0.1\n",
      "694 0.1\n",
      "695 0.1\n",
      "696 0.1\n",
      "697 0.1\n",
      "698 0.1\n",
      "699 0.1\n",
      "700 0.1\n",
      "701 0.1\n",
      "702 0.1\n",
      "703 0.1\n",
      "704 0.1\n",
      "705 0.1\n",
      "706 0.1\n",
      "707 0.1\n",
      "708 0.1\n",
      "709 0.1\n",
      "710 0.1\n",
      "711 0.1\n",
      "712 0.1\n",
      "713 0.1\n",
      "714 0.1\n",
      "715 0.1\n",
      "716 0.1\n",
      "717 0.1\n",
      "718 0.1\n",
      "719 0.1\n",
      "720 0.1\n",
      "721 0.1\n",
      "722 0.1\n",
      "723 0.1\n",
      "724 0.1\n",
      "725 0.1\n",
      "726 0.1\n",
      "727 0.1\n",
      "728 0.1\n",
      "729 0.1\n",
      "730 0.1\n",
      "731 0.1\n",
      "732 0.1\n",
      "733 0.1\n",
      "734 0.1\n",
      "735 0.1\n",
      "736 0.1\n",
      "737 0.1\n",
      "738 0.1\n",
      "739 0.1\n",
      "740 0.1\n",
      "741 0.1\n",
      "742 0.1\n",
      "743 0.1\n",
      "744 0.1\n",
      "745 0.1\n",
      "746 0.1\n",
      "747 0.1\n",
      "748 0.1\n",
      "749 0.1\n",
      "750 0.1\n",
      "751 0.1\n",
      "752 0.1\n",
      "753 0.1\n",
      "754 0.1\n",
      "755 0.1\n",
      "756 0.1\n",
      "757 0.1\n",
      "758 0.1\n",
      "759 0.1\n",
      "760 0.1\n",
      "761 0.1\n",
      "762 0.1\n",
      "763 0.1\n",
      "764 0.1\n",
      "765 0.1\n",
      "766 0.1\n",
      "767 0.1\n",
      "768 0.1\n",
      "769 0.1\n",
      "770 0.1\n",
      "771 0.1\n",
      "772 0.1\n",
      "773 0.1\n",
      "774 0.1\n",
      "775 0.1\n",
      "776 0.1\n",
      "777 0.1\n",
      "778 0.1\n",
      "779 0.1\n",
      "780 0.1\n",
      "781 0.1\n",
      "782 0.1\n",
      "783 0.1\n",
      "784 0.1\n",
      "785 0.1\n",
      "786 0.1\n",
      "787 0.1\n",
      "788 0.1\n",
      "789 0.1\n",
      "790 0.1\n",
      "791 0.1\n",
      "792 0.1\n",
      "793 0.1\n",
      "794 0.1\n",
      "795 0.1\n",
      "796 0.1\n",
      "797 0.1\n",
      "798 0.1\n",
      "799 0.1\n",
      "800 0.1\n",
      "801 0.1\n",
      "802 0.1\n",
      "803 0.1\n",
      "804 0.1\n",
      "805 0.1\n",
      "806 0.1\n",
      "807 0.1\n",
      "808 0.1\n",
      "809 0.1\n",
      "810 0.1\n",
      "811 0.1\n",
      "812 0.1\n",
      "813 0.1\n",
      "814 0.1\n",
      "815 0.1\n",
      "816 0.1\n",
      "817 0.1\n",
      "818 0.1\n",
      "819 0.1\n",
      "820 0.1\n",
      "821 0.1\n",
      "822 0.1\n",
      "823 0.1\n",
      "824 0.1\n",
      "825 0.1\n",
      "826 0.1\n",
      "827 0.1\n",
      "828 0.1\n",
      "829 0.1\n",
      "830 0.1\n",
      "831 0.1\n",
      "832 0.1\n",
      "833 0.1\n",
      "834 0.1\n",
      "835 0.1\n",
      "836 0.1\n",
      "837 0.1\n",
      "838 0.1\n",
      "839 0.1\n",
      "840 0.1\n",
      "841 0.1\n",
      "842 0.1\n",
      "843 0.1\n",
      "844 0.1\n",
      "845 0.1\n",
      "846 0.1\n",
      "847 0.1\n",
      "848 0.1\n",
      "849 0.1\n",
      "850 0.1\n",
      "851 0.1\n",
      "852 0.1\n",
      "853 0.1\n",
      "854 0.1\n",
      "855 0.1\n",
      "856 0.1\n",
      "857 0.1\n",
      "858 0.1\n",
      "859 0.1\n",
      "860 0.1\n",
      "861 0.1\n",
      "862 0.1\n",
      "863 0.1\n",
      "864 0.1\n",
      "865 0.1\n",
      "866 0.1\n",
      "867 0.1\n",
      "868 0.1\n",
      "869 0.1\n",
      "870 0.1\n",
      "871 0.1\n",
      "872 0.1\n",
      "873 0.1\n",
      "874 0.1\n",
      "875 0.1\n",
      "876 0.1\n",
      "877 0.1\n",
      "878 0.1\n",
      "879 0.1\n",
      "880 0.1\n",
      "881 0.1\n",
      "882 0.1\n",
      "883 0.1\n",
      "884 0.1\n",
      "885 0.1\n",
      "886 0.1\n",
      "887 0.1\n",
      "888 0.1\n",
      "889 0.1\n",
      "890 0.1\n",
      "891 0.1\n",
      "892 0.1\n",
      "893 0.1\n",
      "894 0.1\n",
      "895 0.1\n",
      "896 0.1\n",
      "897 0.1\n",
      "898 0.1\n",
      "899 0.1\n",
      "900 0.1\n",
      "901 0.1\n",
      "902 0.1\n",
      "903 0.1\n",
      "904 0.1\n",
      "905 0.1\n",
      "906 0.1\n",
      "907 0.1\n",
      "908 0.1\n",
      "909 0.1\n",
      "910 0.1\n",
      "911 0.1\n",
      "912 0.1\n",
      "913 0.1\n",
      "914 0.1\n",
      "915 0.1\n",
      "916 0.1\n",
      "917 0.1\n",
      "918 0.1\n",
      "919 0.1\n",
      "920 0.1\n",
      "921 0.1\n",
      "922 0.1\n",
      "923 0.1\n",
      "924 0.1\n",
      "925 0.1\n",
      "926 0.1\n",
      "927 0.1\n",
      "928 0.1\n",
      "929 0.1\n",
      "930 0.1\n",
      "931 0.1\n",
      "932 0.1\n",
      "933 0.1\n",
      "934 0.1\n",
      "935 0.1\n",
      "936 0.1\n",
      "937 0.1\n",
      "938 0.1\n",
      "939 0.1\n",
      "940 0.1\n",
      "941 0.1\n",
      "942 0.1\n",
      "943 0.1\n",
      "944 0.1\n",
      "945 0.1\n",
      "946 0.1\n",
      "947 0.1\n",
      "948 0.1\n",
      "949 0.1\n",
      "950 0.1\n",
      "951 0.1\n",
      "952 0.1\n",
      "953 0.1\n",
      "954 0.1\n",
      "955 0.1\n",
      "956 0.1\n",
      "957 0.1\n",
      "958 0.1\n",
      "959 0.1\n",
      "960 0.1\n",
      "961 0.1\n",
      "962 0.1\n",
      "963 0.1\n",
      "964 0.1\n",
      "965 0.1\n",
      "966 0.1\n",
      "967 0.1\n",
      "968 0.1\n",
      "969 0.1\n",
      "970 0.1\n",
      "971 0.1\n",
      "972 0.1\n",
      "973 0.1\n",
      "974 0.1\n",
      "975 0.1\n",
      "976 0.1\n",
      "977 0.1\n",
      "978 0.1\n",
      "979 0.1\n",
      "980 0.1\n",
      "981 0.1\n",
      "982 0.1\n",
      "983 0.1\n",
      "984 0.1\n",
      "985 0.1\n",
      "986 0.1\n",
      "987 0.1\n",
      "988 0.1\n",
      "989 0.1\n",
      "990 0.1\n",
      "991 0.1\n",
      "992 0.1\n",
      "993 0.1\n",
      "994 0.1\n",
      "995 0.1\n",
      "996 0.1\n",
      "997 0.1\n",
      "998 0.1\n",
      "999 0.1\n",
      "1000 0.1\n",
      "1001 0.1\n",
      "1002 0.1\n",
      "1003 0.1\n",
      "1004 0.1\n",
      "1005 0.1\n",
      "1006 0.1\n",
      "1007 0.1\n",
      "1008 0.1\n",
      "1009 0.1\n",
      "1010 0.1\n",
      "1011 0.1\n",
      "1012 0.1\n",
      "1013 0.1\n",
      "1014 0.1\n",
      "1015 0.1\n",
      "1016 0.1\n",
      "1017 0.1\n",
      "1018 0.1\n",
      "1019 0.1\n",
      "1020 0.1\n",
      "1021 0.1\n",
      "1022 0.1\n",
      "1023 0.1\n",
      "1024 0.1\n",
      "1025 0.1\n",
      "1026 0.1\n",
      "1027 0.1\n",
      "1028 0.1\n",
      "1029 0.1\n",
      "1030 0.1\n",
      "1031 0.1\n",
      "1032 0.1\n",
      "1033 0.1\n",
      "1034 0.1\n",
      "1035 0.1\n",
      "1036 0.1\n",
      "1037 0.1\n",
      "1038 0.1\n",
      "1039 0.1\n",
      "1040 0.1\n",
      "1041 0.1\n",
      "1042 0.1\n",
      "1043 0.1\n",
      "1044 0.1\n",
      "1045 0.1\n",
      "1046 0.1\n",
      "1047 0.1\n",
      "1048 0.1\n",
      "1049 0.1\n",
      "1050 0.1\n",
      "1051 0.1\n",
      "1052 0.1\n",
      "1053 0.1\n",
      "1054 0.1\n",
      "1055 0.1\n",
      "1056 0.1\n",
      "1057 0.1\n",
      "1058 0.1\n",
      "1059 0.1\n",
      "1060 0.1\n",
      "1061 0.1\n",
      "1062 0.1\n",
      "1063 0.1\n",
      "1064 0.1\n",
      "1065 0.1\n",
      "1066 0.1\n",
      "1067 0.1\n",
      "1068 0.1\n",
      "1069 0.1\n",
      "1070 0.1\n",
      "1071 0.1\n",
      "1072 0.1\n",
      "1073 0.1\n",
      "1074 0.1\n",
      "1075 0.1\n",
      "1076 0.1\n",
      "1077 0.1\n",
      "1078 0.1\n",
      "1079 0.1\n",
      "1080 0.1\n",
      "1081 0.1\n",
      "1082 0.1\n",
      "1083 0.1\n",
      "1084 0.1\n",
      "1085 0.1\n",
      "1086 0.1\n",
      "1087 0.1\n",
      "1088 0.1\n",
      "1089 0.1\n",
      "1090 0.1\n",
      "1091 0.1\n",
      "1092 0.1\n",
      "1093 0.1\n",
      "1094 0.1\n",
      "1095 0.1\n",
      "1096 0.1\n",
      "1097 0.1\n",
      "1098 0.1\n",
      "1099 0.1\n",
      "1100 0.1\n",
      "1101 0.1\n",
      "1102 0.1\n",
      "1103 0.1\n",
      "1104 0.1\n",
      "1105 0.1\n",
      "1106 0.1\n",
      "1107 0.1\n",
      "1108 0.1\n",
      "1109 0.1\n",
      "1110 0.1\n",
      "1111 0.1\n",
      "1112 0.1\n",
      "1113 0.1\n",
      "1114 0.1\n",
      "1115 0.1\n",
      "1116 0.1\n",
      "1117 0.1\n",
      "1118 0.1\n",
      "1119 0.1\n",
      "1120 0.1\n",
      "1121 0.1\n",
      "1122 0.1\n",
      "1123 0.1\n",
      "1124 0.1\n",
      "1125 0.1\n",
      "1126 0.1\n",
      "1127 0.1\n",
      "1128 0.1\n",
      "1129 0.1\n",
      "1130 0.1\n",
      "1131 0.1\n",
      "1132 0.1\n",
      "1133 0.1\n",
      "1134 0.1\n",
      "1135 0.1\n",
      "1136 0.1\n",
      "1137 0.1\n",
      "1138 0.1\n",
      "1139 0.1\n",
      "1140 0.1\n",
      "1141 0.1\n",
      "1142 0.1\n",
      "1143 0.1\n",
      "1144 0.1\n",
      "1145 0.1\n",
      "1146 0.1\n",
      "1147 0.1\n",
      "1148 0.1\n",
      "1149 0.1\n",
      "1150 0.1\n",
      "1151 0.1\n",
      "1152 0.1\n",
      "1153 0.1\n",
      "1154 0.1\n",
      "1155 0.1\n",
      "1156 0.1\n",
      "1157 0.1\n",
      "1158 0.1\n",
      "1159 0.1\n",
      "1160 0.1\n",
      "1161 0.1\n",
      "1162 0.1\n",
      "1163 0.1\n",
      "1164 0.1\n",
      "1165 0.1\n",
      "1166 0.1\n",
      "1167 0.1\n",
      "1168 0.1\n",
      "1169 0.1\n",
      "1170 0.1\n",
      "1171 0.1\n",
      "1172 0.1\n",
      "1173 0.1\n",
      "1174 0.1\n",
      "1175 0.1\n",
      "1176 0.1\n",
      "1177 0.1\n",
      "1178 0.1\n",
      "1179 0.1\n",
      "1180 0.1\n",
      "1181 0.1\n",
      "1182 0.1\n",
      "1183 0.1\n",
      "1184 0.1\n",
      "1185 0.1\n",
      "1186 0.1\n",
      "1187 0.1\n",
      "1188 0.1\n",
      "1189 0.1\n",
      "1190 0.1\n",
      "1191 0.1\n",
      "1192 0.1\n",
      "1193 0.1\n",
      "1194 0.1\n",
      "1195 0.1\n",
      "1196 0.1\n",
      "1197 0.1\n",
      "1198 0.1\n",
      "1199 0.1\n",
      "1200 0.1\n",
      "1201 0.1\n",
      "1202 0.1\n",
      "1203 0.1\n",
      "1204 0.1\n",
      "1205 0.1\n",
      "1206 0.1\n",
      "1207 0.1\n",
      "1208 0.1\n",
      "1209 0.1\n",
      "1210 0.1\n",
      "1211 0.1\n",
      "1212 0.1\n",
      "1213 0.1\n",
      "1214 0.1\n",
      "1215 0.1\n",
      "1216 0.1\n",
      "1217 0.1\n",
      "1218 0.1\n",
      "1219 0.1\n",
      "1220 0.1\n",
      "1221 0.1\n",
      "1222 0.1\n",
      "1223 0.1\n",
      "1224 0.1\n",
      "1225 0.1\n",
      "1226 0.1\n",
      "1227 0.1\n",
      "1228 0.1\n",
      "1229 0.1\n",
      "1230 0.1\n",
      "1231 0.1\n",
      "1232 0.1\n",
      "1233 0.1\n",
      "1234 0.1\n",
      "1235 0.1\n",
      "1236 0.1\n",
      "1237 0.1\n",
      "1238 0.1\n",
      "1239 0.1\n",
      "1240 0.1\n",
      "1241 0.1\n",
      "1242 0.1\n",
      "1243 0.1\n",
      "1244 0.1\n",
      "1245 0.1\n",
      "1246 0.1\n",
      "1247 0.1\n",
      "1248 0.1\n",
      "1249 0.1\n",
      "1250 0.1\n",
      "1251 0.1\n",
      "1252 0.1\n",
      "1253 0.1\n",
      "1254 0.1\n",
      "1255 0.1\n",
      "1256 0.1\n",
      "1257 0.1\n",
      "1258 0.1\n",
      "1259 0.1\n",
      "1260 0.1\n",
      "1261 0.1\n",
      "1262 0.1\n",
      "1263 0.1\n",
      "1264 0.1\n",
      "1265 0.1\n",
      "1266 0.1\n",
      "1267 0.1\n",
      "1268 0.1\n",
      "1269 0.1\n",
      "1270 0.1\n",
      "1271 0.1\n",
      "1272 0.1\n",
      "1273 0.1\n",
      "1274 0.1\n",
      "1275 0.1\n",
      "1276 0.1\n",
      "1277 0.1\n",
      "1278 0.1\n",
      "1279 0.1\n",
      "1280 0.1\n",
      "1281 0.1\n",
      "1282 0.1\n",
      "1283 0.1\n",
      "1284 0.1\n",
      "1285 0.1\n",
      "1286 0.1\n",
      "1287 0.1\n",
      "1288 0.1\n",
      "1289 0.1\n",
      "1290 0.1\n",
      "1291 0.1\n",
      "1292 0.1\n",
      "1293 0.1\n",
      "1294 0.1\n",
      "1295 0.1\n",
      "1296 0.1\n",
      "1297 0.1\n",
      "1298 0.1\n",
      "1299 0.1\n",
      "1300 0.1\n",
      "1301 0.1\n",
      "1302 0.1\n",
      "1303 0.1\n",
      "1304 0.1\n",
      "1305 0.1\n",
      "1306 0.1\n",
      "1307 0.1\n",
      "1308 0.1\n",
      "1309 0.1\n",
      "1310 0.1\n",
      "1311 0.1\n",
      "1312 0.1\n",
      "1313 0.1\n",
      "1314 0.1\n",
      "1315 0.1\n",
      "1316 0.1\n",
      "1317 0.1\n",
      "1318 0.1\n",
      "1319 0.1\n",
      "1320 0.1\n",
      "1321 0.1\n",
      "1322 0.1\n",
      "1323 0.1\n",
      "1324 0.1\n",
      "1325 0.1\n",
      "1326 0.1\n",
      "1327 0.1\n",
      "1328 0.1\n",
      "1329 0.1\n",
      "1330 0.1\n",
      "1331 0.1\n",
      "1332 0.1\n",
      "1333 0.1\n",
      "1334 0.1\n",
      "1335 0.1\n",
      "1336 0.1\n",
      "1337 0.1\n",
      "1338 0.1\n",
      "1339 0.1\n",
      "1340 0.1\n",
      "1341 0.1\n",
      "1342 0.1\n",
      "1343 0.1\n",
      "1344 0.1\n",
      "1345 0.1\n",
      "1346 0.1\n",
      "1347 0.1\n",
      "1348 0.1\n",
      "1349 0.1\n",
      "1350 0.1\n",
      "1351 0.1\n",
      "1352 0.1\n",
      "1353 0.1\n",
      "1354 0.1\n",
      "1355 0.1\n",
      "1356 0.1\n",
      "1357 0.1\n",
      "1358 0.1\n",
      "1359 0.1\n",
      "1360 0.1\n",
      "1361 0.1\n",
      "1362 0.1\n",
      "1363 0.1\n",
      "1364 0.1\n",
      "1365 0.1\n",
      "1366 0.1\n",
      "1367 0.1\n",
      "1368 0.1\n",
      "1369 0.1\n",
      "1370 0.1\n",
      "1371 0.1\n",
      "1372 0.1\n",
      "1373 0.1\n",
      "1374 0.1\n",
      "1375 0.1\n",
      "1376 0.1\n",
      "1377 0.1\n",
      "1378 0.1\n",
      "1379 0.1\n",
      "1380 0.1\n",
      "1381 0.1\n",
      "1382 0.1\n",
      "1383 0.1\n",
      "1384 0.1\n",
      "1385 0.1\n",
      "1386 0.1\n",
      "1387 0.1\n",
      "1388 0.1\n",
      "1389 0.1\n",
      "1390 0.1\n",
      "1391 0.1\n",
      "1392 0.1\n",
      "1393 0.1\n",
      "1394 0.1\n",
      "1395 0.1\n",
      "1396 0.1\n",
      "1397 0.1\n",
      "1398 0.1\n",
      "1399 0.1\n",
      "1400 0.1\n",
      "1401 0.1\n",
      "1402 0.1\n",
      "1403 0.1\n",
      "1404 0.1\n",
      "1405 0.1\n",
      "1406 0.1\n",
      "1407 0.1\n",
      "1408 0.1\n",
      "1409 0.1\n",
      "1410 0.1\n",
      "1411 0.1\n",
      "1412 0.1\n",
      "1413 0.1\n",
      "1414 0.1\n",
      "1415 0.1\n",
      "1416 0.1\n",
      "1417 0.1\n",
      "1418 0.1\n",
      "1419 0.1\n",
      "1420 0.1\n",
      "1421 0.1\n",
      "1422 0.1\n",
      "1423 0.1\n",
      "1424 0.1\n",
      "1425 0.1\n",
      "1426 0.1\n",
      "1427 0.1\n",
      "1428 0.1\n",
      "1429 0.1\n",
      "1430 0.1\n",
      "1431 0.1\n",
      "1432 0.1\n",
      "1433 0.1\n",
      "1434 0.1\n",
      "1435 0.1\n",
      "1436 0.1\n",
      "1437 0.1\n",
      "1438 0.1\n",
      "1439 0.1\n",
      "1440 0.1\n",
      "1441 0.1\n",
      "1442 0.1\n",
      "1443 0.1\n",
      "1444 0.1\n",
      "1445 0.1\n",
      "1446 0.1\n",
      "1447 0.1\n",
      "1448 0.1\n",
      "1449 0.1\n",
      "1450 0.1\n",
      "1451 0.1\n",
      "1452 0.1\n",
      "1453 0.1\n",
      "1454 0.1\n",
      "1455 0.1\n",
      "1456 0.1\n",
      "1457 0.1\n",
      "1458 0.1\n",
      "1459 0.1\n",
      "1460 0.1\n",
      "1461 0.1\n",
      "1462 0.1\n",
      "1463 0.1\n",
      "1464 0.1\n",
      "1465 0.1\n",
      "1466 0.1\n",
      "1467 0.1\n",
      "1468 0.1\n",
      "1469 0.1\n",
      "1470 0.1\n",
      "1471 0.1\n",
      "1472 0.1\n",
      "1473 0.1\n",
      "1474 0.1\n",
      "1475 0.1\n",
      "1476 0.1\n",
      "1477 0.1\n",
      "1478 0.1\n",
      "1479 0.1\n",
      "1480 0.1\n",
      "1481 0.1\n",
      "1482 0.1\n",
      "1483 0.1\n",
      "1484 0.1\n",
      "1485 0.1\n",
      "1486 0.1\n",
      "1487 0.1\n",
      "1488 0.1\n",
      "1489 0.1\n",
      "1490 0.1\n",
      "1491 0.1\n",
      "1492 0.1\n",
      "1493 0.1\n",
      "1494 0.1\n",
      "1495 0.1\n",
      "1496 0.1\n",
      "1497 0.1\n",
      "1498 0.1\n",
      "1499 0.1\n",
      "1500 0.1\n",
      "1501 0.1\n",
      "1502 0.1\n",
      "1503 0.1\n",
      "1504 0.1\n",
      "1505 0.1\n",
      "1506 0.1\n",
      "1507 0.1\n",
      "1508 0.1\n",
      "1509 0.1\n",
      "1510 0.1\n",
      "1511 0.1\n",
      "1512 0.1\n",
      "1513 0.1\n",
      "1514 0.1\n",
      "1515 0.1\n",
      "1516 0.1\n",
      "1517 0.1\n",
      "1518 0.1\n",
      "1519 0.1\n",
      "1520 0.1\n",
      "1521 0.1\n",
      "1522 0.1\n",
      "1523 0.1\n",
      "1524 0.1\n",
      "1525 0.1\n",
      "1526 0.1\n",
      "1527 0.1\n",
      "1528 0.1\n",
      "1529 0.1\n",
      "1530 0.1\n",
      "1531 0.1\n",
      "1532 0.1\n",
      "1533 0.1\n",
      "1534 0.1\n",
      "1535 0.1\n",
      "1536 0.1\n",
      "1537 0.1\n",
      "1538 0.1\n",
      "1539 0.1\n",
      "1540 0.1\n",
      "1541 0.1\n",
      "1542 0.1\n",
      "1543 0.1\n",
      "1544 0.1\n",
      "1545 0.1\n",
      "1546 0.1\n",
      "1547 0.1\n",
      "1548 0.1\n",
      "1549 0.1\n",
      "1550 0.1\n",
      "1551 0.1\n",
      "1552 0.1\n",
      "1553 0.1\n",
      "1554 0.1\n",
      "1555 0.1\n",
      "1556 0.1\n",
      "1557 0.1\n",
      "1558 0.1\n",
      "1559 0.1\n",
      "1560 0.1\n",
      "1561 0.1\n",
      "1562 0.1\n",
      "1563 0.1\n",
      "1564 0.1\n",
      "1565 0.1\n",
      "1566 0.1\n",
      "1567 0.1\n",
      "1568 0.1\n",
      "1569 0.1\n",
      "1570 0.1\n",
      "1571 0.1\n",
      "1572 0.1\n",
      "1573 0.1\n",
      "1574 0.1\n",
      "1575 0.1\n",
      "1576 0.1\n",
      "1577 0.1\n",
      "1578 0.1\n",
      "1579 0.1\n",
      "1580 0.1\n",
      "1581 0.1\n",
      "1582 0.1\n",
      "1583 0.1\n",
      "1584 0.1\n",
      "1585 0.1\n",
      "1586 0.1\n",
      "1587 0.1\n",
      "1588 0.1\n",
      "1589 0.1\n",
      "1590 0.1\n",
      "1591 0.1\n",
      "1592 0.1\n",
      "1593 0.1\n",
      "1594 0.1\n",
      "1595 0.1\n",
      "1596 0.1\n",
      "1597 0.1\n",
      "1598 0.1\n",
      "1599 0.1\n",
      "1600 0.1\n",
      "1601 0.1\n",
      "1602 0.1\n",
      "1603 0.1\n",
      "1604 0.1\n",
      "1605 0.1\n",
      "1606 0.1\n",
      "1607 0.1\n",
      "1608 0.1\n",
      "1609 0.1\n",
      "1610 0.1\n",
      "1611 0.1\n",
      "1612 0.1\n",
      "1613 0.1\n",
      "1614 0.1\n",
      "1615 0.1\n",
      "1616 0.1\n",
      "1617 0.1\n",
      "1618 0.1\n",
      "1619 0.1\n",
      "1620 0.1\n",
      "1621 0.1\n",
      "1622 0.1\n",
      "1623 0.1\n",
      "1624 0.1\n",
      "1625 0.1\n",
      "1626 0.1\n",
      "1627 0.1\n",
      "1628 0.1\n",
      "1629 0.1\n",
      "1630 0.1\n",
      "1631 0.1\n",
      "1632 0.1\n",
      "1633 0.1\n",
      "1634 0.1\n",
      "1635 0.1\n",
      "1636 0.1\n",
      "1637 0.1\n",
      "1638 0.1\n",
      "1639 0.1\n",
      "1640 0.1\n",
      "1641 0.1\n",
      "1642 0.1\n",
      "1643 0.1\n",
      "1644 0.1\n",
      "1645 0.1\n",
      "1646 0.1\n",
      "1647 0.1\n",
      "1648 0.1\n",
      "1649 0.1\n",
      "1650 0.1\n",
      "1651 0.1\n",
      "1652 0.1\n",
      "1653 0.1\n",
      "1654 0.1\n",
      "1655 0.1\n",
      "1656 0.1\n",
      "1657 0.1\n",
      "1658 0.1\n",
      "1659 0.1\n",
      "1660 0.1\n",
      "1661 0.1\n",
      "1662 0.1\n",
      "1663 0.1\n",
      "1664 0.1\n",
      "1665 0.1\n",
      "1666 0.1\n",
      "1667 0.1\n",
      "1668 0.1\n",
      "1669 0.1\n",
      "1670 0.1\n",
      "1671 0.1\n",
      "1672 0.1\n",
      "1673 0.1\n",
      "1674 0.1\n",
      "1675 0.1\n",
      "1676 0.1\n",
      "1677 0.1\n",
      "1678 0.1\n",
      "1679 0.1\n",
      "1680 0.1\n",
      "1681 0.1\n",
      "1682 0.1\n",
      "1683 0.1\n",
      "1684 0.1\n",
      "1685 0.1\n",
      "1686 0.1\n",
      "1687 0.1\n",
      "1688 0.1\n",
      "1689 0.1\n",
      "1690 0.1\n",
      "1691 0.1\n",
      "1692 0.1\n",
      "1693 0.1\n",
      "1694 0.1\n",
      "1695 0.1\n",
      "1696 0.1\n",
      "1697 0.1\n",
      "1698 0.1\n",
      "1699 0.1\n",
      "1700 0.1\n",
      "1701 0.1\n",
      "1702 0.1\n",
      "1703 0.1\n",
      "1704 0.1\n",
      "1705 0.1\n",
      "1706 0.1\n",
      "1707 0.1\n",
      "1708 0.1\n",
      "1709 0.1\n",
      "1710 0.1\n",
      "1711 0.1\n",
      "1712 0.1\n",
      "1713 0.1\n",
      "1714 0.1\n",
      "1715 0.1\n",
      "1716 0.1\n",
      "1717 0.1\n",
      "1718 0.1\n",
      "1719 0.1\n",
      "1720 0.1\n",
      "1721 0.1\n",
      "1722 0.1\n",
      "1723 0.1\n",
      "1724 0.1\n",
      "1725 0.1\n",
      "1726 0.1\n",
      "1727 0.1\n",
      "1728 0.1\n",
      "1729 0.1\n",
      "1730 0.1\n",
      "1731 0.1\n",
      "1732 0.1\n",
      "1733 0.1\n",
      "1734 0.1\n",
      "1735 0.1\n",
      "1736 0.1\n",
      "1737 0.1\n",
      "1738 0.1\n",
      "1739 0.1\n",
      "1740 0.1\n",
      "1741 0.1\n",
      "1742 0.1\n",
      "1743 0.1\n",
      "1744 0.1\n",
      "1745 0.1\n",
      "1746 0.1\n",
      "1747 0.1\n",
      "1748 0.1\n",
      "1749 0.1\n",
      "1750 0.1\n",
      "1751 0.1\n",
      "1752 0.1\n",
      "1753 0.1\n",
      "1754 0.1\n",
      "1755 0.1\n",
      "1756 0.1\n",
      "1757 0.1\n",
      "1758 0.1\n",
      "1759 0.1\n",
      "1760 0.1\n",
      "1761 0.1\n",
      "1762 0.1\n",
      "1763 0.1\n",
      "1764 0.1\n",
      "1765 0.1\n",
      "1766 0.1\n",
      "1767 0.1\n",
      "1768 0.1\n",
      "1769 0.1\n",
      "1770 0.1\n",
      "1771 0.1\n",
      "1772 0.1\n",
      "1773 0.1\n",
      "1774 0.1\n",
      "1775 0.1\n",
      "1776 0.1\n",
      "1777 0.1\n",
      "1778 0.1\n",
      "1779 0.1\n",
      "1780 0.1\n",
      "1781 0.1\n",
      "1782 0.1\n",
      "1783 0.1\n",
      "1784 0.1\n",
      "1785 0.1\n",
      "1786 0.1\n",
      "1787 0.1\n",
      "1788 0.1\n",
      "1789 0.1\n",
      "1790 0.1\n",
      "1791 0.1\n",
      "1792 0.1\n",
      "1793 0.1\n",
      "1794 0.1\n",
      "1795 0.1\n",
      "1796 0.1\n",
      "1797 0.1\n",
      "1798 0.1\n",
      "1799 0.1\n",
      "1800 0.1\n",
      "1801 0.1\n",
      "1802 0.1\n",
      "1803 0.1\n",
      "1804 0.1\n",
      "1805 0.1\n",
      "1806 0.1\n",
      "1807 0.1\n",
      "1808 0.1\n",
      "1809 0.1\n",
      "1810 0.1\n",
      "1811 0.1\n",
      "1812 0.1\n",
      "1813 0.1\n",
      "1814 0.1\n",
      "1815 0.1\n",
      "1816 0.1\n",
      "1817 0.1\n",
      "1818 0.1\n",
      "1819 0.1\n",
      "1820 0.1\n",
      "1821 0.1\n",
      "1822 0.1\n",
      "1823 0.1\n",
      "1824 0.1\n",
      "1825 0.1\n",
      "1826 0.1\n",
      "1827 0.1\n",
      "1828 0.1\n",
      "1829 0.1\n",
      "1830 0.1\n",
      "1831 0.1\n",
      "1832 0.1\n",
      "1833 0.1\n",
      "1834 0.1\n",
      "1835 0.1\n",
      "1836 0.1\n",
      "1837 0.1\n",
      "1838 0.1\n",
      "1839 0.1\n",
      "1840 0.1\n",
      "1841 0.1\n",
      "1842 0.1\n",
      "1843 0.1\n",
      "1844 0.1\n",
      "1845 0.1\n",
      "1846 0.1\n",
      "1847 0.1\n",
      "1848 0.1\n",
      "1849 0.1\n",
      "1850 0.1\n",
      "1851 0.1\n",
      "1852 0.1\n",
      "1853 0.1\n",
      "1854 0.1\n",
      "1855 0.1\n",
      "1856 0.1\n",
      "1857 0.1\n",
      "1858 0.1\n",
      "1859 0.1\n",
      "1860 0.1\n",
      "1861 0.1\n",
      "1862 0.1\n",
      "1863 0.1\n",
      "1864 0.1\n",
      "1865 0.1\n",
      "1866 0.1\n",
      "1867 0.1\n",
      "1868 0.1\n",
      "1869 0.1\n",
      "1870 0.1\n",
      "1871 0.1\n",
      "1872 0.1\n",
      "1873 0.1\n",
      "1874 0.1\n",
      "1875 0.1\n",
      "1876 0.1\n",
      "1877 0.1\n",
      "1878 0.1\n",
      "1879 0.1\n",
      "1880 0.1\n",
      "1881 0.1\n",
      "1882 0.1\n",
      "1883 0.1\n",
      "1884 0.1\n",
      "1885 0.1\n",
      "1886 0.1\n",
      "1887 0.1\n",
      "1888 0.1\n",
      "1889 0.1\n",
      "1890 0.1\n",
      "1891 0.1\n",
      "1892 0.1\n",
      "1893 0.1\n",
      "1894 0.1\n",
      "1895 0.1\n",
      "1896 0.1\n",
      "1897 0.1\n",
      "1898 0.1\n",
      "1899 0.1\n",
      "1900 0.1\n",
      "1901 0.1\n",
      "1902 0.1\n",
      "1903 0.1\n",
      "1904 0.1\n",
      "1905 0.1\n",
      "1906 0.1\n",
      "1907 0.1\n",
      "1908 0.1\n",
      "1909 0.1\n",
      "1910 0.1\n",
      "1911 0.1\n",
      "1912 0.1\n",
      "1913 0.1\n",
      "1914 0.1\n",
      "1915 0.1\n",
      "1916 0.1\n",
      "1917 0.1\n",
      "1918 0.1\n",
      "1919 0.1\n",
      "1920 0.1\n",
      "1921 0.1\n",
      "1922 0.1\n",
      "1923 0.1\n",
      "1924 0.1\n",
      "1925 0.1\n",
      "1926 0.1\n",
      "1927 0.1\n",
      "1928 0.1\n",
      "1929 0.1\n",
      "1930 0.1\n",
      "1931 0.1\n",
      "1932 0.1\n",
      "1933 0.1\n",
      "1934 0.1\n",
      "1935 0.1\n",
      "1936 0.1\n",
      "1937 0.1\n",
      "1938 0.1\n",
      "1939 0.1\n",
      "1940 0.1\n",
      "1941 0.1\n",
      "1942 0.1\n",
      "1943 0.1\n",
      "1944 0.1\n",
      "1945 0.1\n",
      "1946 0.1\n",
      "1947 0.1\n",
      "1948 0.1\n",
      "1949 0.1\n",
      "1950 0.1\n",
      "1951 0.1\n",
      "1952 0.1\n",
      "1953 0.1\n",
      "1954 0.1\n",
      "1955 0.1\n",
      "1956 0.1\n",
      "1957 0.1\n",
      "1958 0.1\n",
      "1959 0.1\n",
      "1960 0.1\n",
      "1961 0.1\n",
      "1962 0.1\n",
      "1963 0.1\n",
      "1964 0.1\n",
      "1965 0.1\n",
      "1966 0.1\n",
      "1967 0.1\n",
      "1968 0.1\n",
      "1969 0.1\n",
      "1970 0.1\n",
      "1971 0.1\n",
      "1972 0.1\n",
      "1973 0.1\n",
      "1974 0.1\n",
      "1975 0.1\n",
      "1976 0.1\n",
      "1977 0.1\n",
      "1978 0.1\n",
      "1979 0.1\n",
      "1980 0.1\n",
      "1981 0.1\n",
      "1982 0.1\n",
      "1983 0.1\n",
      "1984 0.1\n",
      "1985 0.1\n",
      "1986 0.1\n",
      "1987 0.1\n",
      "1988 0.1\n",
      "1989 0.1\n",
      "1990 0.1\n",
      "1991 0.1\n",
      "1992 0.1\n",
      "1993 0.1\n",
      "1994 0.1\n",
      "1995 0.1\n",
      "1996 0.1\n",
      "1997 0.1\n",
      "1998 0.1\n",
      "1999 0.1\n"
     ]
    }
   ],
   "source": [
    "for episode in range(episodes):\n",
    "    \n",
    "    sigma = max(sigma_end, sigma_decay*sigma) # decrease sigma\n",
    "    \n",
    "    print(episode, sigma)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send models to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg = ddpg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 1.8446524180471897 -0.274514901265502\n",
      "50 1.0324084306135775 -0.2612938731908798\n",
      "60 0.720352313419183 -0.3114666278163592\n",
      "70 0.5536590100731701 -0.3383105706423521\n",
      "80 0.44944866724312305 -0.3492603346705437\n",
      "90 0.3789249758546551 -0.3714071698486805\n",
      "100 0.32856666331312484 -0.38626342692545484\n",
      "110 0.2895795177668333 -0.40027233529835937\n",
      "120 0.2592107468802068 -0.41119028247065015\n",
      "130 0.23501176279969513 -0.4174619419872761\n",
      "140 0.21531412239948458 -0.42633118643002077\n",
      "150 0.19867818478960544 -0.4371251820276181\n",
      "160 0.18438850440658056 -0.44593246109210527\n",
      "170 0.17211432297980148 -0.45589053109288213\n",
      "180 0.16159178625792264 -0.46004232853651045\n",
      "190 0.15223602753248996 -0.46412291331216693\n",
      "200 0.1440647382508306 -0.46910518749671826\n",
      "210 0.1367428654132204 -0.46884195349282687\n",
      "220 0.13030047126996674 -0.4690875327116565\n",
      "230 0.12446538262069225 -0.4718258760124445\n",
      "240 0.11937299746399124 -0.47160908338569457\n",
      "250 0.11465802169404923 -0.47134086638689043\n",
      "260 0.11056420442688725 -0.47144767343997956\n",
      "270 0.10663254590860258 -0.4669290905818343\n",
      "280 0.10302069419622421 -0.4650336486697197\n",
      "290 0.09968194742328845 -0.4602308555004688\n",
      "300 0.09715374002440108 -0.4582289737407808\n",
      "310 0.09432115321646312 -0.4547559357913477\n",
      "320 0.09156359386277096 -0.44691973778194394\n",
      "330 0.08901600684349735 -0.44013367652893065\n",
      "340 0.08670226918954042 -0.4337695906719854\n",
      "350 0.08452028552710544 -0.42768547739833596\n",
      "360 0.08238253752898538 -0.4193389869718389\n",
      "370 0.08038317780558239 -0.4137028954210965\n",
      "380 0.07857418895566037 -0.40888373815055407\n",
      "390 0.07679444261335043 -0.4035408879061126\n",
      "400 0.07508027682810821 -0.4007911157920151\n",
      "410 0.0733873581440237 -0.395668822198518\n",
      "420 0.07177750193036328 -0.3918846658645914\n",
      "430 0.07025809926330112 -0.3888012964138761\n",
      "440 0.06880652877820156 -0.38644329304466163\n",
      "450 0.06739455440103831 -0.38483884496436943\n",
      "460 0.06599418558613505 -0.38368305895304267\n",
      "470 0.06475187972272661 -0.3833758824123916\n",
      "480 0.06350702506697012 -0.38165335905634695\n",
      "490 0.06232866680694987 -0.381303938044964\n",
      "500 0.06117640810404369 -0.3803132573856001\n",
      "510 0.060077195491370125 -0.37922189200374606\n",
      "520 0.05900024613745663 -0.3785455782992803\n",
      "530 0.05798942983243614 -0.37936230035498736\n",
      "540 0.056976696139420656 -0.3786299609999154\n",
      "550 0.05601081326066588 -0.3777887081619925\n",
      "560 0.055118976039636246 -0.3759066292453768\n",
      "570 0.05427035352266911 -0.37505261037429727\n",
      "580 0.053439320351251145 -0.37458423498340626\n",
      "590 0.05262249210666466 -0.37368783826074964\n",
      "600 0.05182109328656735 -0.372618756250462\n",
      "610 0.05104636181113792 -0.3728645210411271\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "\n",
    "for episode in range(episodes):\n",
    "\n",
    "    env_info = env.reset(train_mode=True)[brain_name]    \n",
    "    states = env_info.vector_observations                 \n",
    "    scores = np.zeros(num_agents)                          \n",
    "    \n",
    "    losses_critic = []\n",
    "    losses_actor = []\n",
    "    \n",
    "    sigma = max(sigma_end, sigma_decay*sigma) # decrease sigma\n",
    "        \n",
    "    while True:\n",
    "        \n",
    "        action, q = ddpg.forward(s=torch.from_numpy(states).float().to(device))\n",
    "            \n",
    "         # Add noise\n",
    "        action = action.detach().cpu().numpy()\n",
    "        action = action + np.random.normal(loc=0.0, scale=sigma, size=action.shape)\n",
    "        action = np.clip(action, -1, 1)\n",
    "        \n",
    "        env_info = env.step(action)[brain_name] \n",
    "        \n",
    "        next_states = env_info.vector_observations\n",
    "        \n",
    "        rewards = env_info.rewards                      \n",
    "        dones = env_info.local_done                       \n",
    "        scores += env_info.rewards                         \n",
    "    \n",
    "        if np.any(dones):                                 \n",
    "            break\n",
    "            \n",
    "        replaybuffer.add(\n",
    "            state=states,\n",
    "            state_prime=next_states,\n",
    "            reward=np.array(rewards),\n",
    "            action=action,\n",
    "            weight=None,\n",
    "            done=np.array(dones)\n",
    "        )\n",
    "            \n",
    "        states = next_states\n",
    "        \n",
    "        for i in range(update_steps):\n",
    "            \n",
    "            iteration +=1 \n",
    "        \n",
    "            (\n",
    "                state, \n",
    "                state_prime,\n",
    "                action,\n",
    "                reward,\n",
    "                done, \n",
    "                weight\n",
    "            ) = replaybuffer.draw(batch_size, replace=False)\n",
    "\n",
    "            if state is not None:\n",
    "\n",
    "                cl, al = ddpg.loss(\n",
    "                    gamma=gamma,\n",
    "                    s=torch.from_numpy(state).float().to(device),\n",
    "                    sprime=torch.from_numpy(state_prime).float().to(device),\n",
    "                    r=torch.from_numpy(reward).float().to(device),\n",
    "                    a=torch.from_numpy(action).float().to(device),\n",
    "                    d=torch.from_numpy(done).float().to(device),\n",
    "                    w=torch.from_numpy(weight).float().to(device),\n",
    "                )\n",
    "\n",
    "                ddpg.update(cl=cl, al=al)\n",
    "\n",
    "                losses_critic.append(cl.item())\n",
    "                losses_actor.append(al.item())\n",
    "                \n",
    "                if iteration % 10 == 0:\n",
    "                    print(iteration, np.mean(losses_critic), np.mean(losses_actor))\n",
    "                \n",
    "    writer.add_scalar(\"training/kpi/score\", np.mean(scores), episode) # The score\n",
    "    writer.add_scalar(\"training/kpi/loss_actor\", np.mean(losses_actor), episode) # The actor loss\n",
    "    writer.add_scalar(\"training/kpi/loss_critic\", np.mean(losses_critic), episode) # The critic loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 4)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 33)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer.draw(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 33])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.draw(200).state_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'replay_buffer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-822a486d0033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'replay_buffer' is not defined"
     ]
    }
   ],
   "source": [
    "len(replay_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2530439ff707>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# select an action (for each agent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# all actions between -1 and 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m           \u001b[0;31m# send all actions to tne environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m         \u001b[0;31m# get next state (for each agent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m                         \u001b[0;31m# get reward (for each agent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             outputs = self.communicator.exchange(\n\u001b[0;32m--> 369\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             )\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
